{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BRSR Principle 6 — Faithfulness Mapping Pipeline\n",
        "\n",
        "This notebook is a **ready-to-run template** that implements the minimal end-to-end pipeline for the assignment:\n",
        "- Load SEBI BRSR rules PDF and a company's BRSR PDF\n",
        "- Chunk documents and build vector index (FAISS)\n",
        "- Run Retrieval-Augmented Generation (RAG) using an LLM to compare SEBI requirements vs company disclosures\n",
        "- Produce drift scores (0–3), Sankey diagram, drift dashboard\n",
        "- Export a Word report containing findings and visuals\n",
        "\n",
        "**Important:** This notebook is a template. You must install the required Python packages and provide API keys (if using OpenAI). Instructions are below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment & Installation\n",
        "\n",
        "Run this (once) in your notebook environment (Colab / local / VS Code terminal):\n",
        "```bash\n",
        "# Create venv (optional)\n",
        "python -m venv .venv\n",
        "source .venv/bin/activate  # (or .venv\\Scripts\\activate on Windows)\n",
        "pip install --upgrade pip\n",
        "pip install openai faiss-cpu sentence-transformers python-docx PyPDF2 matplotlib scikit-learn tqdm\n",
        "# Optional (if you prefer LangChain)\n",
        "pip install langchain\n",
        "```\n",
        "If you want to use **local embeddings** (no OpenAI), the notebook uses `sentence-transformers`.\n",
        "\n",
        "If you prefer OpenAI embeddings or OpenAI LLMs, set your API key as an environment variable:\n",
        "```bash\n",
        "export OPENAI_API_KEY='sk-...'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Required Files\n",
        "\n",
        "Place these PDFs in the same folder as this notebook or provide paths when prompted:\n",
        "- `sebi_brsr.pdf`  (SEBI BRSR Annexure / guidance PDF)\n",
        "- `company_brsr.pdf` (e.g., Infosys/TCS/Wipro BRSR PDF)\n",
        "\n",
        "This notebook will create an `outputs/` folder where it stores images and the final Word doc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "outputs/ created\n"
          ]
        }
      ],
      "source": [
        "# Create outputs folder\n",
        "from pathlib import Path\n",
        "Path('outputs').mkdir(exist_ok=True)\n",
        "print('outputs/ created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Simple PDF loader & extractor\n",
        "This uses `PyPDF2` to extract text. For more robust extraction, consider `unstructured` or `pdfplumber`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NOTE: replace sebi_brsr.pdf and company_brsr.pdf with your local file paths before running\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "def extract_text_from_pdf(path):\n",
        "    text = []\n",
        "    with open(path, 'rb') as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for i, page in enumerate(reader.pages):\n",
        "            try:\n",
        "                page_text = page.extract_text() or ''\n",
        "            except Exception as e:\n",
        "                page_text = ''\n",
        "            text.append(page_text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "# Example usage (replace with your actual file paths)\n",
        "sebi_pdf = 'SEBIpdf.pdf'\n",
        "company_pdf = 'infosis.pdf'\n",
        "print('NOTE: replace sebi_brsr.pdf and company_brsr.pdf with your local file paths before running')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Text chunking\n",
        "We split long text into fixed-size chunks with some overlap so retrieval works better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks: 6\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def simple_chunk_text(text, chunk_size=800, overlap=100):\n",
        "    # sanitize\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        chunk = text[i:i+chunk_size]\n",
        "        chunks.append(chunk)\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Small demo\n",
        "sample = 'This is a long text. ' * 200\n",
        "print('Chunks:', len(simple_chunk_text(sample)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Embeddings\n",
        "Two options:\n",
        "1. **OpenAI embeddings** (requires OPENAI_API_KEY) — convenient and high-quality.\n",
        "2. **SentenceTransformers** local model — works offline (smaller and slower).\n",
        "Choose one and follow the cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding setup: USE_OPENAI_EMBEDDINGS= False USE_LOCAL_EMBEDDINGS= True\n"
          ]
        }
      ],
      "source": [
        "# Option A: OpenAI embeddings (comment out if not using)\n",
        "USE_OPENAI_EMBEDDINGS = False\n",
        "OPENAI_MODEL_EMBED = 'text-embedding-3-small'\n",
        "\n",
        "# Option B: local sentence-transformers\n",
        "USE_LOCAL_EMBEDDINGS = True\n",
        "LOCAL_EMBED_MODEL = 'all-MiniLM-L6-v2'\n",
        "\n",
        "print('Embedding setup: USE_OPENAI_EMBEDDINGS=', USE_OPENAI_EMBEDDINGS, 'USE_LOCAL_EMBEDDINGS=', USE_LOCAL_EMBEDDINGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded local embedding model: all-MiniLM-L6-v2\n"
          ]
        }
      ],
      "source": [
        "if USE_LOCAL_EMBEDDINGS:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    embedder = SentenceTransformer(LOCAL_EMBED_MODEL)\n",
        "    def get_embeddings(texts):\n",
        "        # returns list of vectors\n",
        "        return embedder.encode(texts, show_progress_bar=True)\n",
        "    print('Loaded local embedding model:', LOCAL_EMBED_MODEL)\n",
        "else:\n",
        "    print('Using OpenAI embeddings (you must set OPENAI_API_KEY in environment)')\n",
        "    import openai\n",
        "    def get_embeddings(texts):\n",
        "        res = openai.Embedding.create(model=OPENAI_MODEL_EMBED, input=texts)\n",
        "        return [r['embedding'] for r in res['data']]\n",
        "    print('OpenAI embeddings ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build FAISS index\n",
        "We will embed chunks and index them with FAISS for fast nearest-neighbor retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS functions ready\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def build_faiss_index(chunks, embed_fn):\n",
        "    vectors = embed_fn(chunks)\n",
        "    dim = len(vectors[0])\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(np.array(vectors).astype('float32'))\n",
        "    return index, np.array(vectors).astype('float32')\n",
        "\n",
        "def faiss_search(index, vectors, query_vec, k=4):\n",
        "    D, I = index.search(np.array([query_vec]).astype('float32'), k)\n",
        "    return I[0], D[0]\n",
        "\n",
        "print('FAISS functions ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Retrieval helper\n",
        "Given a query (SEBI requirement) retrieve the most relevant chunks from the company report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval helper ready\n"
          ]
        }
      ],
      "source": [
        "def retrieve_chunks(query, index, vectors, chunks, embed_fn, k=4):\n",
        "    qv = embed_fn([query])[0]\n",
        "    idxs, dists = faiss_search(index, vectors, qv, k=k)\n",
        "    results = [{'chunk': chunks[i], 'score': float(dists[j]), 'index': int(idxs[j])} for j,i in enumerate(idxs) if i!=-1]\n",
        "    return results\n",
        "\n",
        "print('Retrieval helper ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. LLM Comparison Prompt Template\n",
        "We will send the SEBI requirement + retrieved company chunks to an LLM and ask for:\n",
        "1. A short mapping explanation\n",
        "2. A drift score (0–3) with short justification\n",
        "3. Evidence quotes (company chunks & page numbers if available)\n",
        "\n",
        "Example prompt is below. This notebook uses OpenAI's text completion (or chat) API, but you can adapt it to any LLM provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM prompt/template ready. Set LLM_TYPE and OPENAI_API_KEY as required.\n"
          ]
        }
      ],
      "source": [
        "LLM_TYPE = 'openai'  # 'openai' or 'mock' (mock returns example outputs for offline testing)\n",
        "OPENAI_CHAT_MODEL = 'gpt-4o-mini'  # replace with your model\n",
        "\n",
        "def build_prompt(sebi_requirement, retrieved_chunks):\n",
        "    prompt = []\n",
        "    prompt.append('You are an auditor comparing a regulation requirement to company disclosures.')\n",
        "    prompt.append('SEBI requirement:\\n' + sebi_requirement)\n",
        "    prompt.append('\\n---\\nRetrieved company evidence (most relevant chunks):')\n",
        "    for i, r in enumerate(retrieved_chunks):\n",
        "        snippet = r['chunk'][:800]\n",
        "        prompt.append(f'\\n--- Chunk {i+1} (score:{r[\"score\"]}):\\n{snippet}\\n')\n",
        "    prompt.append('\\nTask:')\n",
        "    prompt.append('1) Provide a one-paragraph mapping between the SEBI requirement and the company evidence.')\n",
        "    prompt.append('2) Assign a drift score (0,1,2,3). Output only the number and then a one-line justification.')\n",
        "    prompt.append('3) Provide up to two short direct quotes from the evidence that support your judgment.')\n",
        "    prompt.append('4) Provide a short machine-readable JSON with keys: {\"score\": <int>, \"justification\": <str>, \"quotes\": [..]}')\n",
        "    return '\\n'.join(prompt)\n",
        "\n",
        "def call_llm(prompt):\n",
        "    if LLM_TYPE == 'mock':\n",
        "        # simple deterministic mock answer for offline testing\n",
        "        return {\n",
        "            'score': 0,\n",
        "            'justification': 'Numbers match SEBI expectation; detailed tables present',\n",
        "            'quotes': ['Total electricity consumption 712,134 GJ', 'Total Scope 1 emissions 8,593 tCO2e']\n",
        "        }\n",
        "    else:\n",
        "        import openai, os, json\n",
        "        openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "        if not openai.api_key:\n",
        "            raise ValueError('OPENAI_API_KEY not set in environment. Set it and rerun or use mock mode.')\n",
        "        # Use chat completions\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=OPENAI_CHAT_MODEL,\n",
        "            messages=[{'role':'system','content':'You are a concise auditor.'},{'role':'user','content':prompt}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        text = resp['choices'][0]['message']['content']\n",
        "        # try to parse JSON at the end of the response\n",
        "        # we assume the assistant returns the required JSON object at the end of its text\n",
        "        # attempt to find a JSON substring\n",
        "        import re\n",
        "        m = re.search(r\"(\\{\\s*\\\"score\\\".*\\})\", text, re.S)\n",
        "        if m:\n",
        "            j = json.loads(m.group(1))\n",
        "            return j\n",
        "        else:\n",
        "            # fallback: return entire text as justification\n",
        "            return {'score': None, 'justification': text, 'quotes': []}\n",
        "\n",
        "print('LLM prompt/template ready. Set LLM_TYPE and OPENAI_API_KEY as required.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. High-level pipeline function\n",
        "This function ties everything together for a list of SEBI requirements (we will extract key Principle 6 items manually or from the SEBI PDF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "analyze_requirements function ready\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def analyze_requirements(requirements, company_chunks, company_index, vectors, embed_fn, top_k=4):\n",
        "    results = []\n",
        "    for req in requirements:\n",
        "        retrieved = retrieve_chunks(req, company_index, vectors, company_chunks, embed_fn, k=top_k)\n",
        "        prompt = build_prompt(req, retrieved)\n",
        "        try:\n",
        "            llm_out = call_llm(prompt)\n",
        "        except Exception as e:\n",
        "            print('LLM call failed:', e)\n",
        "            llm_out = {'score': None, 'justification': str(e), 'quotes': []}\n",
        "        results.append({'requirement': req, 'retrieved': retrieved, 'llm': llm_out})\n",
        "    return results\n",
        "\n",
        "print('analyze_requirements function ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualization helpers (Sankey & Drift dashboard)\n",
        "These create and save images to `outputs/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visualization helpers ready\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.sankey import Sankey\n",
        "def create_sankey(mapping_pairs, out_path='outputs/sankey.png'):\n",
        "    # mapping_pairs: list of (left_label, right_label, weight)\n",
        "    labels_left = [p[0] for p in mapping_pairs]\n",
        "    labels_right = [p[1] for p in mapping_pairs]\n",
        "    flows = [p[2] for p in mapping_pairs]\n",
        "    fig = plt.figure(figsize=(8,5))\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    S = Sankey(ax=ax, unit=None)\n",
        "    for i, f in enumerate(flows):\n",
        "        try:\n",
        "            S.add(flows=[f, -f], labels=[labels_left[i], labels_right[i]], orientations=[0,0], trunklength=1.0, pathlengths=[0.25,0.25])\n",
        "        except Exception:\n",
        "            pass\n",
        "    S.finish()\n",
        "    plt.title('Sankey: Principle 6 concepts → Company evidence (illustrative)')\n",
        "    plt.savefig(out_path, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    return out_path\n",
        "\n",
        "def create_drift_dashboard(results, out_path='outputs/drift_dashboard.png'):\n",
        "    names = [r['requirement'][:60] for r in results]\n",
        "    scores = [r['llm'].get('score', 3) if r['llm'].get('score') is not None else 3 for r in results]\n",
        "    color_map = {0:'#2ca02c', 1:'#98df8a', 2:'#ff7f0e', 3:'#d62728'}\n",
        "    colors = [color_map.get(int(s), '#7f7f7f') for s in scores]\n",
        "    fig, ax = plt.subplots(figsize=(10, max(4, len(names)*0.4)))\n",
        "    bars = ax.barh(range(len(names)), scores, color=colors)\n",
        "    ax.set_yticks(range(len(names)))\n",
        "    ax.set_yticklabels(names, fontsize=9)\n",
        "    ax.set_xlim(-0.2,3.5)\n",
        "    ax.set_xlabel('Drift score (0 = verbatim, 3 = vague/performative)')\n",
        "    ax.invert_yaxis()\n",
        "    for i, v in enumerate(scores):\n",
        "        ax.text(v + 0.05, i, str(v), va='center')\n",
        "    plt.title('Drift Dashboard — Principle 6')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    return out_path\n",
        "\n",
        "print('Visualization helpers ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Word report generator\n",
        "This creates a `.docx` containing the summary, a drift table, and embeds the images produced above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word report generator ready\n"
          ]
        }
      ],
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt, Inches\n",
        "def create_word_report(results, sankey_path='outputs/sankey.png', drift_path='outputs/drift_dashboard.png', out_path='outputs/brsr_faithfulness_report.docx'):\n",
        "    doc = Document()\n",
        "    doc.styles['Normal'].font.name = 'Calibri'\n",
        "    doc.styles['Normal'].font.size = Pt(11)\n",
        "    doc.add_heading('Faithful Concept Mapper — BRSR Principle 6', level=1)\n",
        "    doc.add_paragraph('This report is generated by the BRSR Faithfulness Mapping pipeline.')\n",
        "    doc.add_heading('Summary', level=2)\n",
        "    good = sum(1 for r in results if r['llm'].get('score')==0)\n",
        "    doc.add_paragraph(f'Number of requirements analyzed: {len(results)}. Number with score 0 (verbatim): {good}.')\n",
        "    doc.add_heading('Drift Table', level=2)\n",
        "    table = doc.add_table(rows=1, cols=4)\n",
        "    hdr = table.rows[0].cells\n",
        "    hdr[0].text = 'Requirement'\n",
        "    hdr[1].text = 'Drift score'\n",
        "    hdr[2].text = 'Justification'\n",
        "    hdr[3].text = 'Quotes'\n",
        "    for r in results:\n",
        "        row = table.add_row().cells\n",
        "        row[0].text = r['requirement']\n",
        "        row[1].text = str(r['llm'].get('score'))\n",
        "        row[2].text = r['llm'].get('justification','')[:500]\n",
        "        row[3].text = '\\n'.join(r['llm'].get('quotes',[]))[:500]\n",
        "    doc.add_heading('Visuals', level=2)\n",
        "    try:\n",
        "        doc.add_picture(sankey_path, width=Inches(6))\n",
        "    except Exception:\n",
        "        doc.add_paragraph('Sankey image not available')\n",
        "    try:\n",
        "        doc.add_picture(drift_path, width=Inches(6))\n",
        "    except Exception:\n",
        "        doc.add_paragraph('Drift dashboard not available')\n",
        "    doc.save(out_path)\n",
        "    return out_path\n",
        "\n",
        "print('Word report generator ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Example: Minimal run-through (mock mode)\n",
        "If you do not want to call a real LLM while testing, set `LLM_TYPE = 'mock'` earlier. Below is an example that runs the entire pipeline in mock mode using toy text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.80it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.99it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.17it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.99it/s]\n",
            "Ignoring fixed y limits to fulfill fixed data aspect with adjustable data limits.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Demo outputs created:\n",
            " - outputs/sankey.png\n",
            " - outputs/drift_dashboard.png\n",
            " - outputs/brsr_faithfulness_report.docx\n"
          ]
        }
      ],
      "source": [
        "# Demo run (mock)\n",
        "LLM_TYPE = 'mock'\n",
        "sebi_text = 'PRINCIPLE 6: Report total energy consumption, energy intensity, Scope 1 and Scope 2 GHG emissions, water withdrawal and water intensity, waste management by category.'\n",
        "company_text = (\n",
        "    'Total electricity consumption 712134 GJ. Total fuel consumption 38852 GJ. Energy intensity 5.11 GJ per crore. '\n",
        "    'Scope 1 emissions 8593 tCO2e. Scope 2 emissions 62352 tCO2e. '\\\n",
        "    'Water withdrawal 2274679 kl. Water intensity 15.5 kl per crore. '\n",
        "    'Waste: 1200 t general waste, 800 t recycled.'\n",
        ")\n",
        "sebi_chunks = simple_chunk_text(sebi_text, chunk_size=400, overlap=50)\n",
        "company_chunks = simple_chunk_text(company_text, chunk_size=400, overlap=50)\n",
        "company_index, vectors = build_faiss_index(company_chunks, get_embeddings)\n",
        "requirements = [\n",
        "    'Total energy consumption and energy intensity',\n",
        "    'Scope 1 and Scope 2 GHG emissions and intensity',\n",
        "    'Water withdrawal and water intensity',\n",
        "    'Waste management by category and recycling rates'\n",
        "]\n",
        "results = analyze_requirements(requirements, company_chunks, company_index, vectors, get_embeddings, top_k=2)\n",
        "sankey_path = create_sankey([\n",
        "    ('Energy', 'Energy tables', 1),\n",
        "    ('GHG', 'Scope 1 & 2 tables', 1),\n",
        "    ('Water', 'Water tables', 1),\n",
        "    ('Waste', 'Waste tables', 1)\n",
        "], out_path='outputs/sankey.png')\n",
        "drift_path = create_drift_dashboard(results, out_path='outputs/drift_dashboard.png')\n",
        "doc_path = create_word_report(results, sankey_path=sankey_path, drift_path=drift_path, out_path='outputs/brsr_faithfulness_report.docx')\n",
        "print('Demo outputs created:')\n",
        "print(' -', sankey_path)\n",
        "print(' -', drift_path)\n",
        "print(' -', doc_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Next steps (when you run with real PDFs)\n",
        "1. Replace `sebi_brsr.pdf` and `company_brsr.pdf` paths with your real files.\n",
        "2. Extract text with `extract_text_from_pdf` and chunk with `simple_chunk_text`.\n",
        "3. Build embeddings (set `USE_LOCAL_EMBEDDINGS` or `USE_OPENAI_EMBEDDINGS`).\n",
        "4. Build FAISS index for the company report chunks.\n",
        "5. Prepare a list of SEBI Principle 6 requirements (you can manually type the essential & leadership indicators from SEBI PDF).\n",
        "6. Run `analyze_requirements(...)` to get results.\n",
        "7. Generate visuals and Word report.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
